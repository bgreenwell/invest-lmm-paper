%\VignetteEngine{knitr::knitr}
%\VignetteDepends{RColorBrewer}
%\VignetteDepends{lattice}
%\VignetteDepends{nlme}
%\VignetteDepends{investr}
%\VignetteDepends{car}
%\VignetteDepends{lme4}
%\VignetteDepends{boot}
%\VignetteIndexEntry{Inverse estimation with linear mixed-effects models}

\documentclass[article]%{jss}

%% Packages
\usepackage{amsmath, bm, framed, booktabs}

%% Macros
\newcommand{\boot}{\ensuremath{^\star}}
\newcommand{\trans}{\ensuremath{^\top}}
\newcommand{\SE}{\operatorname{SE}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\col}{\operatorname{col}}
\newcommand{\row}{\operatorname{row}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\author{Brandon M. Greenwell\\Aptima, Inc \And
        Christine M. Schubert Kabban\\Air Force Institute of Technology}
\title{Inverse Estimation with Linear Mixed-Effects Models in \proglang{R}}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Brandon M. Greenwell, Christine M. Schubert Kabban} %% comma-separated
\Plaintitle{Linear Calibration with Random Coefficients via the R Packages investr and lme4} %% without formatting
\Shorttitle{Linear Calibration with Random Coefficients} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{
  Inverse estimation, more commonly known as calibration, is a classical and well-known problem in regression. In simple terms, it involves the use of an observed value of the response (or specified value of the mean response) to make inference on the corresponding unknown value of the explanatory variable.  In this paper, we describe how to calculate approximate calibration confidence intervals for the unknown value of the explanatory variable in random coefficient models using a well-known data set.
}
\Keywords{calibration, random coefficients, \pkg{investr}, \pkg{lme4}, \pkg{nlme}, \proglang{R}}
\Plainkeywords{calibration, random coefficients, investr, lme4, nlme, R} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Brandon M. Greenwell\\
  Associate Research Engineer\\
  Aptima, Inc.\\
  3100 Presidential Drive, Suite 220\\
  Fairborn, OH 45324\\
  E-mail: \email{greenwell.brandon@gmail.com}\\
  URL: \url{https://github.com/w108bmg/investr}
%   URL: \url{http://www.aptima.com/about/aptima-team/brandon-greenwell}
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/512/507-7103
%% Fax: +43/512/507-2851

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%% include your article here, just as usual
%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

%% Knitr stuff %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
<<setup, cache=FALSE, include=FALSE>>=
## Set global chunk options
options(replace.assign = TRUE, width = 90, digits = 3)
# render_sweave()
# opts_chunk$set(comment = NA, highlight = FALSE, background = '#FFFFFF')
# global chunk options
opts_chunk$set(cache=TRUE, message=FALSE, warning=FALSE)

## Set up colors for plotting
library(RColorBrewer)
cols <- brewer.pal(9, "Set1")

## Bladder data
subject <- rep(1:23, times = 8)
volume <- rep(c(10, 25, 50, 75, 100, 125, 150, 175), each = 23) / 10
HD <- c(13.2, 11.1, 10.3, NA, 4.8, 7.7, NA, 5.9, 1.9, 6.5, 19.8,
        14.6, NA, NA, 9.7, 17.2, 10.6, 19.3, 8.5, 6.9, 8.1, 14.8, 13.7,
        27.4, 27.5, 15, 10, 18.6, 12.6, 24, 28.4, 12.5, 16.7, 29.6,
        27.1, 14, 18.7, 20.3, 35.8, 23.6, 37.4, 31.3, 23.7, 22, 34.3,
        28.5, 41.6, 58.1, 34.2, 28.8, 29.9, 31.4, 46.9, 44.4, 26.8,
        30.6, 51.7, 49.8, 19.1, 35.8, 38.9, 41.4, 49.9, 58.6, 54.8, 44,
        39.1, 58.5, 41.5, 60.1, 78.8, 49.4, 46.4, 39.4, 45.3, 50.4,
        70.7, 54.4, 41.8, 72.2, 67.5, 39.2, 49.6, 65.1, 69.7, 67.7,
        73.7, 78.3, 65.7, 44.7, 72.1, 59.8, 73.9, 91.5, 71.3, 54.8, NA,
        48, 67.8, 89.4, 63.1, 49.6, 81.9, 79.1, 48.7, 65.6, 65.1, 81.9,
        87.7, 79.4, 93, 80.3, 68.9, 90.9, 77.5, 85.5, 98.3, 81.3, 69.4,
        NA, 66.6, 81, 105.8, 83.5, 60.8, 95.1, 95.1, 67, 85.3, 86.9,
        96.6, 89.3, 102.6, NA, 93.6, 93.3, 105, 92.9, 95.6, 111.4, 94,
        73.9, NA, NA, 91.2, 113.5, 114.5, 80.1, 115.4, 109.8, 72.7,
        90.4, 98.6, 115, 108, 110.9, NA, 99.2, 102.4, 117.5, 99.4,
        107.4, 121, 104.3, NA, NA, NA, 99.8, 127.3, 124, 87.1, NA, NA,
        NA, NA, 107.2, 117, 114.8, 122.4, NA, 112.2, 104.7, 124.2, 113)
bladder <- data.frame(subject = subject, HD = HD, HD2 = HD^(3/2),
                      volume = volume)
bladder <- na.omit(bladder)
@



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%% Note: If there is markup in \(sub)section, then it has to be escape as above.

Consider an ordinary regression model $\mathcal{Y}_i = f\left( x_i; \bm{\beta} \right) + \epsilon_i$ $(i = 1, \dotsc, n)$, where $f$ is a known expectation function (called a \emph{calibration curve}) that is monotonic over the range of interest and $\epsilon_i \stackrel{iid}{\sim} \mathcal{N}\left( 0, \sigma^2 \right)$.  A common problem in regression is to predict a future response $\mathcal{Y}_0$ (or estimate the mean response $f_0$) for a known value of the explanatory variable $x_0$.  Often, however, there is a need to do the reverse; that is, given an observed value of the response $\mathcal{Y} = y_0$ (or a specified value of the mean response), estimate the unknown value of the explanatory variable $x_0$.  This is known as the \emph{calibration problem}, though it is often refered to more generally as inverse estimation.  A thorough overview of the calibration problem is given in \citet{osborne-statistical-1991}.  \citet{oman-calibration-1998} considers the case of a random intercept and slope model --- the goal of this paper is to show how easily this can be accomplished using a few packages from the \proglang{R} programming language.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Linear Mixed-Effects Models}

Linear regression models with random coefficients can be represented in many different (but equivalent) forms.  One of the most common forms, attributed to \citet{laird-random-1982}, is
\begin{equation}
\label{eqn:lmm-laird-and-ware}
  \bm{\mathcal{Y}}_i = \bm{X}_i\bm{\beta} + \bm{Z}_i\bm{b}_i + \bm{\epsilon}_i, \quad i = 1, \dotsc, m,
\end{equation}
where
\begin{itemize}
  \item $\bm{\mathcal{Y}}_i$ is an $n_i \times 1$ response vector for the $i$-th subject/cluster/group;
  \item $\bm{X}_i$ is an $n_i \times p$ design matrix for the fixed effects;
  \item $\bm{Z}_i$ is an $n_i \times q$ design matrix for the random effects;
  \item $\bm{\beta}$ is a $p \times 1$ vector of fixed effects coefficients;
  \item $\bm{b}_i$ is a $q \times 1$ vector of random effects coefficients with mean zero and variance-covariance matrix $\bm{D}$;
  \item $\bm{D}$ is a $q \times q$ variance-covariance matrix for the random effects;
  \item $\bm{\epsilon}_i$ is an $n_i \times 1$ vector of random errors with mean zero and variance-covariance matrix $\sigma^2\bm{I}$.
\end{itemize}
Equation~\eqref{eqn:lmm-laird-and-ware} is known as a linear mixed-effects model (LMM).  The random effects and errors are often assumed to follow a normal distribution.  By stacking the data, the (normal) LMM can be written concisely as
\begin{equation}
\label{eqn:lmm-stacked}
    \bm{\mathcal{Y}} = \bm{X}\bm{\beta} + \bm{Z}\bm{b} + \bm{\epsilon}, \quad
      \begin{bmatrix}
        \bm{b} \\
        \bm{\epsilon}
      \end{bmatrix} \sim
      \mathcal{N}\left(\begin{bmatrix}
        \bm{0} \\
        \bm{0}
      \end{bmatrix}, \begin{bmatrix}
        \bm{D} & \bm{0} \\
        \bm{0} & \sigma^2\bm{I}
      \end{bmatrix}\right),
\end{equation}
where $\bm{\mathcal{Y}} = \col\left\{\bm{\mathcal{Y}}_i\right\}$, $\bm{X} = \col\left\{\bm{X}_i\right\}$, $\bm{Z} = \diag\left\{\bm{Z}_i\right\}$, $\bm{b} = \col\left\{\bm{b}_i\right\}$, and $\bm{\epsilon} = \col\left\{\bm{\epsilon}_i\right\}$ for $i = 1, \dotsc, m$.  Since $\COV\left[\bm{b}, \bm{\epsilon}\right] = \bm{0}$, it is assumed that the random vectors $\big\{ \bm{b}_i, \bm{\epsilon}_i \big\}_{i=1}^m$ are mutually independent.

The additional term $\bm{Z}\bm{b}$ in the model imposes a specific variance-covariance structure on the response vector $\bm{\mathcal{Y}}$:
\[
  \bm{\mathcal{Y}} \sim \mathcal{N}\left(\bm{X}\bm{\beta}, \bm{V}\right), \quad \bm{V} = \bm{Z}\bm{D}\bm{Z}\trans + \sigma^2\bm{I}.
\]
Thus, the fixed effects determine the mean of $\bm{\mathcal{Y}}$, while the random effects govern the variance-covariance structure of $\bm{\mathcal{Y}}$.  Different random effects structures impose different variance-covariance structures on the response resulting in a highly flexible framework for modelling \emph{grouped data}.

The random effects variance-covariance matrix $\bm{D}$ has at most $q(q+1)/2$ unique elements which we represent by the vector $\bm{\theta}$.  There are a number of methods available for estimating $\left( \bm{\beta}, \sigma^2, \bm{\theta} \right)$; see, for example, \citet[chap 6.]{mcculloch_generalized_2008} and \citet[chap. 2]{demidenko_mixed_2013}.  Most commonly, the fixed-effects $\bm{\beta}$ are estimated via the method of maximum likelihood (ML), while the variance components $\left(\sigma^2, \bm{\theta}\right)$ are estimated via restricted maximum likelihood (REML).  The ML estimator of $\bm{\beta}$, given by
\begin{equation*}
\widehat{\bm{\beta}} = \left( \bm{X}\trans \widehat{\bm{V}}^{-1} \bm{X} \right)\bm{X}\trans\bm{\mathcal{Y}},
\end{equation*}
depends on the estimated variance components through $\widehat{\bm{V}}$ which makes it difficult to capture the variability of $\widehat{\bm{\beta}}$ in small sample sizes (see \citet[pp. 165-167]{mcculloch_generalized_2008}).  The usual practice is to ignore the variability of the estimated variance components when making inference about the fixed effects; that is, treat $\widehat{\bm{V}}$ as the true (fixed) value of $\bm{V}$.  Modern computational procedures such as the parametric bootstrap and Markov chain Monte Carlo (MCMC) methods are two ways account for the variability of the estimated variance components.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bladder Volume Data}
For illustration, let us consider the bladder volume data which can be found in \citet[pg. 7]{brown-measurement-1993} and \citet{oman-calibration-1998}.  In Brown's words:
\begin{quotation}
\noindent``A series of 23 women patients attending a urodynamic clinic were recruited for the study. After successful voiding of the bladder, sterile water was introduced in additions of 1, 1.5, and then 2.5 cl increments up to a final cumulative total of 17.5 cl. At each volume a measure of height (\code{H}) in mm and depth (\code{D}) in mm of largest ultrasound bladder images were taken. The product \code{H} $\times$ \code{D} was taken as a measure of liquid volume.''
\end{quotation}
We took Brown's suggestion and transformed the data so that the relationship is approximately linear.  Spaghettiplots of both the original and transformed data are displayed in Figure~\ref{fig:spaghetti}.

<<spaghetti, echo=FALSE, fig.width=7, fig.height=4, out.width='\\linewidth', fig.cap='Spaghettiplot of bladder volume data (lines connect measurements belonging to the same subject). \\textit{Left}: Original data. \\textit{Right}: Transformed data.', fig.pos='!htb'>>=
library(lattice)
p1 <- xyplot(HD ~ volume, groups = subject, type = c("g", "b"), data = bladder,
             scales = list(tck = c(1, 0)), cex = 0.5, alpha = 1,
             xlab = "Volume (cl)")
p2 <- xyplot(HD^(3/2) ~ volume, groups = subject, type = c("g", "b"), data = bladder,
             scales = list(tck = c(1, 0)), cex = 0.5, alpha = 1,
             xlab = "Volume (cl)",
             ylab = expression(HD^{3/2}))
print(p1, position = c(0, 0, 0.5, 1), more = TRUE)
print(p2, position = c(0.5, 0, 1, 1))
@

A random intercept and slope model with uncorrelated random effects fits the transformed data well:
\begin{align*}
  \texttt{HD}^{3/2}_{ij} &= \left(\beta_0 + b_{0i}\right) + \left(\beta_1 + b_{1i}\right)\texttt{volume}_{ij} + \epsilon_{ij} \\
  b_{ki} &\sim \mathcal{N}\left(0, \theta_k^2\right), \quad k = 0, 1, \\
  \epsilon_{ij} &\sim \mathcal{N}\left(0, \sigma^2\right),
\end{align*}
where $\COV\left[b_{0i}, b_{1i}\right] = 0$.  To fit such a model in \proglang{R}, we can use the recommended \pkg{nlme} package \citep{pinheiro-nlme-2013} as follows:
<<>>=
library(nlme)  # nlme should already be installed
(fit.nlme <- lme(HD^(3/2) ~ volume, random = list(subject = pdDiag(~volume)),
                 data = bladder))
@
The \code{pdDiag} function forces a diagonal variance-covariance structure on the random effects; hence, a covariance of zero. The same model, but with an additional quadratic term, provides a useful fit to the original (untransformed) data:
\begin{align*}
  \texttt{HD}_{ij} &= \left(\beta_0 + b_{0i}\right) + \left(\beta_1 + b_{1i}\right)\texttt{volume}_{ij} + \beta_2\texttt{volume}_{ij}^2 + \epsilon_{ij} \\
  b_{ki} &\sim \mathcal{N}\left(0, \theta_k^2\right), \quad k = 0, 1, \\
  \epsilon_{ij} &\sim \mathcal{N}\left(0, \sigma^2\right).
\end{align*}
For an in-depth treatment on fitting LMMs using the \pkg{nlme} software, see \citet{pinheiro-mixed-2000}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Extending the Classical Methods of Calibration}
The standard methods of calibration, (i.e., the Wald-based and inversion confidence intervals) are easily extended to the case of random coefficients.  For convenience, let us write the linear random coefficient model as
\begin{equation*}
  \mathcal{Y}_{ij} = f\left(x_{ij}; \bm{\beta}\right) + R\left(x_{ij}; \bm{b}_i\right) + \epsilon_{ij},
\end{equation*}
where $f(\cdot)$ and $R(\cdot)$ are linear in $\bm{\beta}$ and $\bm{b}_i$, respectively.  For instance, the model for the transformed bladder data has $f\left(\texttt{volume}_{ij}; \bm{\beta}\right) = \beta_0 + \beta_1 \texttt{volume}_{ij}$ and $R\left(\texttt{volume}_{ij}; \bm{b}_i\right) = b_{0i} + b_{1i}\texttt{volume}_{ij}$ with $\VAR\left[R\left(\texttt{volume}_{ij}; \bm{b}_i\right)\right] = \theta_0^2 + \texttt{volume}_{ij}^2\theta_1^2$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Point Estimation}
Assume that, after the data are collected and a model is fitted, we obtain a new observation, denoted $\mathcal{Y}_0$, from the same population under study for which the value of the explanatory variable $x_0$ is unknown.  If we assume that the new observation belongs to a group not included in our analysis then estimating $x_0$ is rather straightforward.  By assumption, the new observation $\mathcal{Y}_0$ is distributed as a $\mathcal{N}\left\{f\left(x_0; \bm{\beta}\right), \sigma_0^2\right\}$ random variable with $\sigma_0^2 = \VAR\left[ R\left( x_0; \bm{b}_0 \right) \right ] + \sigma^2$.  A natural estimator for $x_0$ is then
\begin{equation}
\label{eqn:est}
  \widehat{x}_0 = f^{-1}\left(\mathcal{Y}_0; \widehat{\bm{\beta}}\right),
\end{equation}
where $\widehat{\bm{\beta}}$ is the ML estimator of $\bm{\beta}$.  We shall refer to Equation~\eqref{eqn:est} as the classical estimator.  Note that the point estimate $\widehat{x}_0$ does not involve any of the random effects; the random effects only contribute to the variance-covariance structure of the response.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{An Asymptotic Wald Interval}
\label{sec:wald}
An approximate $100(1-\alpha)\%$ Wald-type confidence interval for $x_0$ has the simple form
\begin{equation}
\label{eqn:wald}
  CI_{wald}\left(x_0\right)\left(x_0\right) = \left( \widehat{x}_0 -  \SE\left[\widehat{x}_0\right]\Phi\left(\alpha/2\right),  \widehat{x}_0 -  \SE\left[\widehat{x}_0\right]\Phi\left(1-\alpha/2\right) \right).
\end{equation}
There is no ``textbook'' formula for the standard error of $\widehat{x}_0$, instead, an estimate of this standard error is obtained using a first-order Taylor series approximation, or better yet, a bootstrap approximation.  For the Taylor series approximation, we need the variance-covariance matrix of $\left(\mathcal{Y}_0, \widehat{\bm{\beta}}\right)$,
\[
\Sigma = \begin{bmatrix}
           \VAR\left[\mathcal{Y}_0\right] & \bm{0} \\
           \bm{0} & \VAR\left[\widehat{\bm{\beta}}\right]
         \end{bmatrix} = \begin{bmatrix}
           \sigma_0^2 & \bm{0} \\
           \bm{0} & \left(\bm{X}\trans\bm{V}^{-1}\bm{X}\right)^{-1}
         \end{bmatrix}.
\]
Since $\mathcal{Y}_0$ is independent of $\bm{\mathcal{Y}}$, it is also independent of $\widehat{\bm{\beta}}$, hence the diagonal structure of $\Sigma$. Recall that our point estimate has the form $x = f^{-1}\left(y; \bm{\beta}\right)$. Let $f_1^{-1}\left(y; \bm{\beta}\right)$ and $f_2^{-1}\left(y; \bm{\beta}\right)$ denote the partial derivatives of $f^{-1}$ with respect to the parameters $y$ and $\bm{\beta}$, respectively. Our point estimator is given by $f^{-1}\left(\mathcal{Y}_0; \widehat{\bm{\beta}}\right)$, where $\mathcal{Y}_0$ is a new observation and $\widehat{\bm{\beta}}$ is the ML estimator of $\bm{\beta}$.  A first-order Taylor-series approximation for the variance of $\widehat{x}_0$ is given by
\begin{align}
  \VAR\left[\widehat{x}_0\right] &= \left[f_1^{-1}\left(\mathcal{Y}_0; \widehat{\bm{\beta}}\right)\right]^2\sigma_0^2 \nonumber \\
   &+ \left[f_2^{-1}\left(\mathcal{Y}_0; \widehat{\bm{\beta}}\right)\right]\trans\left(\bm{X}\trans\bm{V}^{-1}\bm{X}\right)^{-1}\left[f_2^{-1}\left(\mathcal{Y}_0; \widehat{\bm{\beta}}\right)\right].
\end{align}
To obtain $\SE\left[\widehat{x}_0\right] = \Big\{ \widehat{\VAR}\left[\widehat{x}_0\right] \Big\}^{1/2}$, we simply replace $\sigma_0^2$ and $\bm{V}$ with their respective estimates $\widehat{\sigma}_0^2$ and $\widehat{\bm{V}}$.

The Wald-based interval is simple to compute as long as we have an estimate for the standard error.  As we will discuss in Section~\ref{sec:implementation}, the \proglang{R} package \pkg{investr} \citep{investr-package} can be used to obtain the Wald-based interval \eqref{eqn:wald} using a Taylor series approximation of the standard error.  If a closed-form formula is available for $\widehat{x}_0$, then the \code{deltaMethod} function from the \pkg{car} package \citep{fox-car-2011} can also be used to obtain the Taylor series approximation of the standard error.  Alternatively, one can use a parametric bootstrap estimate of the standard error instead of relying on a Taylor series approximation --- see Section~\ref{sec:percentile}.  The bootstrap estimate may be more accurate in smaller sample sizes because, unlike the Taylor approximation estimate, it takes into account the variability of the estimated variance components.  The new \code{bootMer} function in the \pkg{lme4} package \citep{bates-lme4-2014} can be used for model-based parametric bootstrapping in mixed models.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{An Asymptotic Inversion Interval}
\label{sec:inversion}
In the case of the simple linear regression model with constant variance, an exact $100(1-\alpha)\%$ confidence interval for $x_0$ can be derived \citep{graybill-theory-1976}.  This can be generalized to an approximate method in the case of polynomial or nonlinear regression models with independent observations and constant variance (see \citet{seber-nonlinear-2003} and \citet{huet-statistical-2004}).  In a similar fashion, we can generalize the same results to an approximate method for random coefficient models.  Let $\widehat{f}_0 = f\left(x_0; \widehat{\bm{\beta}}\right)$ be the predicted mean at $x = x_0$.  A prediction interval for $\mathcal{Y}_0$ at $x_0$ with asymptotic coverage probability $100(1-\alpha)\%$ is
\begin{equation}
\label{eqn:asymptotic-pi}
  \mathcal{I}_\infty\left(x_0\right) = \widehat{f}_0 \pm z_{1-\alpha/2}\left\{ \widehat{\VAR}\left[\mathcal{Y}_0 - \widehat{f}_0\right] \right\}^{1/2}.
\end{equation}
If instead, $\mathcal{Y}_0$ is observed to be $y_0$ and $x_0$ is unknown, then an asymptotic $100(1-\alpha)\%$ confidence interval for the unknown $x_0$ can be obtained by inverting \eqref{eqn:asymptotic-pi}:
\begin{equation}
\label{eqn:inversion}
  CI_{inv}\left(x_0\right) = \left\{ x: z_{\alpha/2} \le \frac{\mathcal{Y}_0-f\left(x; \widehat{\bm{\beta}}\right)}{\left\{ \widehat{\VAR}\left[\mathcal{Y}_0 - f\left(x; \widehat{\bm{\beta}}\right)\right] \right\}^{1/2}} \le z_{1-\alpha/2} \right\}.
\end{equation}
This is known as the \emph{inversion interval} and typically cannot be written in closed-form; therefore, numerical techniques are required to find the lower and upper bounds.  Further, note that $CI_{inv}\left(x_0\right)$ is not symmetric about $\widehat{x}_0$ and will not necessarily result in a single finite interval.

Fortunately, the inversion interval \eqref{eqn:inversion} can be computed automatically using the \pkg{investr} package.  However, like the Wald-based interval \eqref{eqn:wald}, the inversion interval ignores the variability of the estimated variance components and will likely perform poorly in small sample sizes.  An alternative approach involving the parametric bootstrap will be discussed in Section~\ref{sec:parboot-inversion}.

Finally, the inversion interval uses a normal approximation.  While it is likely that a $t$~distribution with some finite degrees of freedom may be more accurate, it is difficult to find the appropriate degrees of freedom. \citet{oman-calibration-1998}, suggests a $t$~distribution with $N-1$ degrees of freedom ($N$ being the total sample size).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A Parametric Bootstrap Algorithm}
\label{sec:parboot}
The bootstrap \citep{efron-bootstrap-1979} is a general-purpose computer-based method for assessing accuracy of estimators and forming confidence intervals for parameters.  \citet{jones-bootstrapping-1999} proposed a nonparametric bootstrap algorithm for controlled calibration with independent observations.  However, since our application involves random coefficients (i.e., dependent observations), the nonparametric bootstrap does not easily apply, and instead, we adopt a ``fully parametric'' approach.  In a parametric bootstrap, bootstrap samples are generated from a fitted parametric model rather than sampling with replacement directly from the data.  Fortunately, parametric bootstrap confidence intervals are usually more accurate than nonparametric ones, however, by sampling from a fitted parametric family, we are implicitly assuming that we have the ``correct model''.

Let $\widehat{\sigma}_0^2$ be an estimate of the variance of the new observation $\mathcal{Y}_0$.  An algorithm for bootstrapping $\widehat{x}_0$ in an LMM is given in Figure~\ref{fig:parboot}.  Note that step 5. is crucial for calibration problems because we need to treat $y_0$ as a random quantity in the bootstrap simulation, otherwise the variability of $\widehat{x}_0$ will be underestimated; see, for example, \citet{jones-bootstrapping-1999} and \citet{greenwell-investr-2014}.
\begin{figure}[!htb]
\begin{framed}
\begin{enumerate}
  \item Fit a mixed model \eqref{eqn:lmm-stacked} to the data and obtain estimates $\widehat{\bm{\beta}}$, $\widehat{\bm{D}}$, and $\widehat{\sigma}^2$.
	\item Define $\bm{y}\boot = \bm{X}\widehat{\bm{\beta}} + \bm{Z}\bm{b}\boot + \bm{\epsilon}\boot$, where $\bm{b}\boot \sim \mathcal{N}_q\left(\bm{0}, \widehat{\bm{D}}\right)$ and $\bm{\epsilon}\boot \sim \mathcal{N}_N\left(\bm{0}, \widehat{\sigma}_\epsilon^2\bm{I}\right)$;
	\item Update the original model using $\bm{y}\boot$ as the response vector to obtain $\widehat{\bm{\beta}}\boot$ and $\widehat{\sigma}_0^{2\star}$;
	\item Generate $y_0\boot \sim \mathcal{N}\left(y_0, \widehat{\sigma}_0^{2\star}\right)$;
	\item Define $\widehat{x}_0\boot = f^{-1}\left(y_0\boot; \widehat{\bm{\beta}}\boot\right)$;
  \item Repeat steps (2)-(5) $R$ times.
\end{enumerate}
\end{framed}
\caption{Parametric bootstrap algorithm for linear calibration with random coefficients. \label{fig:parboot}}
\end{figure}

There are three main bootstrap confidence interval procedures: The percentile methods introduced in \citet{efron-bootstrap-1979}, the studentized bootstrap $t$ method introduced in \citet{efron-jackknife-1982}, and the double bootstrap method \citep{hall-bootstrap-1986}.  In this paper, we focus on the simple percentile interval and the studentized method.  For a good overview of all these confidence interval procedures, see \citet[chap. 5]{hinkley-bootstrap-1997} and \citet[chap. 11]{boos-essential-2013}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The percentile interval}
\label{sec:percentile}
The percentile method is the simplest and is given by the sample $\alpha/2$ and $1-\alpha/2$ quantiles of the bootstrap sample.  Let $\widehat{x}_{1*}, \dotsc, \widehat{x}_{R*}$ be a bootstrap sample obtained from the algorithm in Figure~\ref{fig:parboot}.  If $\widehat{F}_R$ is the empirical distribution function of the bootstrap sample, then an approximate $100(1-\alpha)\%$ confidence interval for $x_0$ is given by
\[
  \left( \widehat{F}_R^{-1}(\alpha/2), \widehat{F}_R^{-1}(1-\alpha/2) \right).
\]
The percentile interval is transformation respecting; thus, if we want a confidence interval for any one-to-one transformation $g\left(\widehat{x}_0\right)$, then we can just apply the transformation to the endpoints of the percentile interval for $\widehat{x}_0$.  The drawback is that the percentile interval is only \emph{first-order accurate} (see \citet[pp. 429-430]{boos-essential-2013}).  \citet{efron-better-1987} offered an improvement over the percentile interval called the \emph{bias-corrected and accelerated} ($BC_a$) interval that often obtains second-order accuracy and is transformation respecting.  However, the recommended \proglang{R} package \pkg{boot} \citep{canty-boot-2013} we will be relying on currently does not allow for $BC_a$ confidence intervals to be constructed from a parametric bootstrap.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The bootstrap $t$ interval}
\label{sec:student}
A bootstrap $t$ interval for $x_0$ is essentially a bootstrap adjusted Wald-type interval.  The Wald interval for $x_0$ \eqref{eqn:wald} assumes that
\[
  Q_W = \frac{\widehat{x}_0 - x_0}{\SE\left[\widehat{x}_0\right]} \sim \mathcal{N}(0, 1).
\]
Rather than assuming that $Q_W$ is normal, the bootstrap $t$ method uses the bootstrap distribution of $Q_W\boot = \left(\widehat{x}_0\boot - \widehat{x}_0\right)/\SE\left[\widehat{x}_0\boot\right]$ to estimate the true distribution of $Q_W$.  If $\widehat{F}_{Q_W}$ is the empirical distribution function for a sample of $R$ bootstrap replicates of $Q_W$, then the bootstrap $t$  interval for $x_0$ is given by
\[
    CI_{wald}\left(x_0\right)\boot = \left( \widehat{x}_0 -  \SE\left[\widehat{x}_0\right]\widehat{F}_{Q_W}\left(\alpha/2\right),  \widehat{x}_0 -  \SE\left[\widehat{x}_0\right]\widehat{F}_{Q_W}\left(1-\alpha/2\right) \right).
\]
In order to implement this method, we need to calculate the standard error of each bootstrap replicate.  To do this, we can either use an additional (nested) bootstrap to estimate the standard error, or use a Taylor series approximation as discussed in Section~\ref{sec:wald}.  If time is  not a factor, then the nested bootstrap is preferred since bootstrap standard errors are usually more accurate than those based on a first-order Taylor series approximation \citep[pp. 478-480]{casella-statistical-2002}.  The benefits of using this interval over \eqref{eqn:wald} are that (a) it does not assume normality (hence, likely to be more accurate in smaller sample sizes) and (b) it is not symmetric about $\widehat{x}_0$; thus, more realistic when the response is nonlinear in $x$ (e.g., polynomials, etc.).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bootstrap adjusted inversion interval}
\label{sec:parboot-inversion}
\citet{huet-statistical-2004} suggests a bootstrap modification of the usual inversion interval in nonlinear regression models with dependent data.  In a similar fashion, we could use the parametric bootstrap to adjust the approximate inversion interval given in Equation~\eqref{eqn:inversion}.  The inversion interval assumes that the \emph{predictive pivot}
\[
  Q_I = \frac{\mathcal{Y}_0-f\left(x; \widehat{\bm{\beta}}\right)}{\left\{ \widehat{\VAR}\left[\mathcal{Y}_0 - f\left(x; \widehat{\bm{\beta}}\right)\right] \right\}^{1/2}} \sim \mathcal{N}(0, 1).
\]
A bootstrap modified inversion interval would then use the bootstrap distribution of
\[
  Q_I\boot = \frac{\mathcal{Y}_0\boot-f\left(\widehat{x}_0; \widehat{\bm{\beta}}\boot\right)}{\left\{ \widehat{\VAR}\left[\mathcal{Y}_0 - f\left(\widehat{x}_0; \widehat{\bm{\beta}}\boot\right)\right] \right\}^{1/2}},
\]
to estimate the true distribution of $Q_I$.  If $\widehat{F}_{Q_I}$ is the empirical distribution function for a sample of $R$ bootstrap replicates of $Q_I$, then the modified inversion interval for $x_0$ is given by
\[
    CI_{inv}\left(x_0\right)\boot = \left\{ x: \widehat{F}_{Q_I}\left(\alpha/2\right) \le Q_I \le \widehat{F}_{Q_I}\left(1-\alpha/2\right) \right\}.
\]
\vspace{25pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section[Implementation in R]{Implementation in \proglang{R}}
\label{sec:implementation}
Here, we discuss how to implement the previous procedures in the \proglang{R} programming languages.  We discuss two main packages: \pkg{investr} and \pkg{lme4}.  The \pkg{investr} package can be used for obtaining the Wald-based and inversion intervals.  This functionality is demonstrated over the next two sections.  The \pkg{lme4} package is a popular package for fitting linear, generalized linear, and nonlinear mixed models. Recently, however, the \pkg{lme4} package creators have added functionality for model-based parametric bootstrapping.  In Section~\ref{sec:lme4}, we demonstrate the potential of this new functionality by applying our parametric bootstrap algorithm to the bladder volume data discussed earlier.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection[The investr package]{The \pkg{investr} package}
The \proglang{R} package \pkg{investr} facilitates calibration/inverse estimation with linear and nonlinear regression models.  The main function, \code{invest}, can be used for inverse estimation of $x_0$ given an observed response $y_0$.  More recently, the package has been updated to also handle objects of class \code{lme} from the \pkg{nlme} package.  Current functionality includes both the Wald-based and inversion methods outlined in Sections \ref{sec:wald}-\ref{sec:inversion}.  The main arguments for this function (as it applies to \code{lme} objects) are noted in Table~\ref{tab:invest} below.  The code for the package is hosted on GitHub at \url{https://github.com/w108bmg/investr}, but the latest stable release can be found on CRAN at \url{http://CRAN.R-project.org/package=investr}.

\begin{table}[!htb]
\begin{tabular}{lp{12cm}}
  \toprule
  Argument    & Description \\
  \midrule
  \code{object}        & An \proglang{R} object that inherits from class \code{lm}, \code{nls}, or \code{lme}. \\
  \code{y0}            & The value of the observed response. \\
  \code{interval}      & The type of interval required. Currently, only \code{"none"}, \code{"Wald"}, and \code{"inversion"} are supported. \\
  \code{level}         & A numeric scalar between 0 and 1 giving the confidence level for the interval to be calculated. The default is \code{0.95}. \\
  \code{lower}         & The lower endpoint of the interval to be searched. \\
  \code{upper} 				 & The upper endpoint of the interval to be searched. \\
  \code{q1}, \code{q2} & Optional quantiles to be used for constructing confidence intervals. \\
  \code{tol}           & The desired accuracy to be passed on to \code{uniroot}. \\
  \code{maxiter}       & The maximum number of iterations to be passed on to \code{uniroot}. \\
  \bottomrule
\end{tabular}
\caption{Main arguments for the \code{invest} function. \label{tab:invest}}
\end{table}

Returning to the bladder volume example, suppose we obtained an ultrasound measurement from a new patient for which $\texttt{HD}^{3/2} = 500$ (that's roughly 63 on the original scale).  What is the true volume of fluid ($x_0$) in the patients bladder?  We can estimate the true volume and form an approximate 95\% confidence interval using the methods discussed previously.  The point estimate is simply given by
\[
  \widehat{x}_0 = \frac{500 + 53.83164}{69.09491} = 8.0155 \text{ (cl)}.
\]
This estimate can be obtained in \proglang{R} as follows:
<<>>=
library(investr)
(x0.est <- invest(fit.nlme, y0 = 500, interval = "none"))
@
The code used by \code{invest} to obtain this point estimate is basically
<<>>=
fun <- function(x) {
  predict(fit.nlme, newdata = list("volume" = x), level = 0) - 500
}
uniroot(fun, lower = 1, upper = 17.5, tol = 1e-10, maxiter = 1000)$root
@
In other words, \code{invest} relies on the function \code{uniroot} from the \pkg{stats} package to solve the equation $f\left(x; \widehat{\bm{\beta}}\right) - y_0 = 0$ numerically for $x$.  If the solution does not lie in the range of predictor values, then an error message will be displayed, as in
<<error>>=
invest(fit.nlme, y0 = 1500)
@
The values for \code{lower}, \code{upper}, \code{tol}, and \code{maxiter} are controlled via the arguments of the same name listed in Table~\ref{tab:invest}.

When \code{interval = "Wald"}, an asymptotic $100(1-\alpha)\%$ confidence interval (where $\alpha$ is equal to \code{1 - level}) for $x_0$ is calculated according to Equation~\eqref{eqn:wald}:
<<>>=
invest(fit.nlme, y0 = 500, interval = "Wald")
@
The standard error is computed using a First-order Taylor series approximation.   Similar to the code snippet shown below, \code{invest} calls the \pkg{stats} function \code{numericDeriv} to numerically evaluate the gradient of $\widehat{x}_0$ as a function of $y_0$ and $\widehat{\bm{\beta}}$.
<<>>=
dmFun <- function(params) { # function of parameters whose gradient is required
  fun <- function(x) {
    X <- model.matrix(eval(fit.nlme$call$fixed)[-2],
                      data = data.frame("volume" = x))
    X %*% params[-length(params)] - params[length(params)]
  }
  uniroot(fun, lower = 1, upper = 17.5, tol = 1e-10,
          maxiter = 1000)$root
}
params <- c(fixef(fit.nlme), 500)
covmat <- diag(3)  # set up variance-covariance matrix
covmat[1:2, 1:2] <- vcov(fit.nlme)  # fixed effects var/cov matrix
covmat[3, 3] <- 17572.35  # VAR[Y_0]
gv <- attr(numericDeriv(quote(dmFun(params)), "params"), "gradient")
(se <- as.numeric(sqrt(gv %*% covmat %*% t(gv))))
@

Alternatively, one can use the very useful \code{deltaMethod} function from the \pkg{car} package to obtain \code{se}:
<<>>=
library(car)  # assuming package car is already installed
params <- c(fixef(fit.nlme), 500)
covmat <- diag(3)  # set up var/cov matrix
covmat[1:2, 1:2] <- vcov(fit.nlme)  # fixed effects var/cov matrix
covmat[3, 3] <-  17572.35  # VAR[Y_0]
names(params) <- c("b0", "b1", "y0")
(se <- deltaMethod(params, g = "(y0 - b0)/b1",  vcov. = covmat)$SE)
@
The only drawback here is that \code{deltaMethod} relies on the \pkg{stats} package symbolic differentiation function \code{D}; hence, $\widehat{x}_0 = f^{-1}\left(y_0; \widehat{\bm{\beta}}\right)$ has to be obtainable in closed-form.

To obtain the approximate inversion interval \eqref{eqn:inversion}, we specify \code{interval = "inversion"} (the default) as in the following:
<<>>=
invest(fit.nlme, y0 = 500, interval = "inversion")
@
Essentially, \code{invest} finds the lower and upper inversion confidence limits \eqref{eqn:inversion} by solving the equations
\begin{equation*}
  Q_I - z_{\alpha/2} = 0 \quad \text{and} \quad Q_I - z_{1-\alpha/2} = 0
\end{equation*}
numerically for $x$ using the \proglang{R} function \code{uniroot}.  To use the quantiles from a $t$~distribution instead (see Section~\ref{sec:inversion}), we can supply them via the arguments \code{q1} and \code{q2}:
<<>>=
N <- nrow(bladder)  # total sample size
tvals <- qt(c(0.025, 0.975), df = N-1)  # quantiles from t distribution
invest(fit.nlme, y0 = 500, q1 = tvals[1], q2 = tvals[2])
@
Being able to specify the arguments \code{q1} and \code{q2} will also be useful when implementing the bootstrap adjusted inversion interval described in Section~\ref{sec:parboot}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Monte Carlo Study}
To assess the empirical performance of these confidence intervals, we carried out a small Monte Carlo study. The simulations described below were conducted in \proglang{R} using packages \pkg{plyr} \citep{wickham-plyr-2011}, \pkg{nlme}, and \pkg{lme4} \citep{bates-lme4-2014}.  The results are reported in Table~\ref{tab:simulation} and indicate that the Wald-based confidence interval \eqref{eqn:wald} and inversion confidence interval \eqref{eqn:inversion} have asymptotic coverage probability close to $100(1-\alpha)\%$.  This experiment also highlighted the fact that it is the number of subject $m$, not the sample size per subject $n$, that is more important for good asymptotic coverage.  The code used for the simulation is available upon request.

We consider the values 5, 10, 30, 50, and 100 for both the number of subjects $m$ and and the number of observations per subject $n$.  For each combination of sample sizes, we generated 1,000 data sets from a random intercept and slope model with parameters given by those listed in \code{summary(fit.nlme)}.  In other words, the fixed effects were $\bm{\beta} = \left(-53.83164, 69.09491\right)\trans$, the standard deviations for the (uncorrelated) random intercept and slope were 39.62499, and 14.28841, respectively.  The residual standard deviation was $\sigma = 53.71511$.  We chose $f\left(x_0; \bm{\beta}\right) = 500$ so that the true unknown is $x_0 = 8.0155$.  The standard deviation of the coverage estimates is approximately $\sqrt{0.95\left(1-0.95\right)/1000} = 0.001$.  A trellis plot of the results is given in Figure~\ref{fig:simulation}.  The coverage estimates are plotted against the number of subjects $m$ and paneled by number of observations per subject $n$.
\begin{table}[!htb]
\centering
\begin{tabular}{llccccc}
  \toprule
  %\rule{0pt}{4ex}
  $m$  & Method    & \hspace{8pt}$n = 5$\hspace{8pt} & \hspace{8pt}$n = 10$\hspace{8pt} & \hspace{8pt}$n = 30$\hspace{8pt} & \hspace{8pt}$n = 50$\hspace{8pt} & \hspace{8pt}$n = 100$ \hspace{8pt} \\
  \hline
  %\rule{0pt}{4ex}
  5    & Wald      & 0.89    & 0.89     & 0.89     & 0.87     & 0.89      \\
       & Inversion & 0.89    & 0.90     & 0.89     & 0.88     & 0.90      \\ \hline
  10   & Wald      & 0.92    & 0.92     & 0.94     & 0.93     & 0.93      \\
       & Inversion & 0.92    & 0.92     & 0.94     & 0.94     & 0.93      \\ \hline
  30   & Wald      & 0.95    & 0.95     & 0.95     & 0.94     & 0.95      \\
       & Inversion & 0.94    & 0.94     & 0.94     & 0.94     & 0.95      \\ \hline
  50   & Wald      & 0.95    & 0.96     & 0.95     & 0.94     & 0.94      \\
       & Inversion & 0.94    & 0.95     & 0.95     & 0.94     & 0.94      \\ \hline
  100  & Wald      & 0.95    & 0.95     & 0.95     & 0.94     & 0.94      \\
       & Inversion & 0.95    & 0.95     & 0.95     & 0.94     & 0.95      \\
  \bottomrule
\end{tabular}
\caption{Coverage probability of 95\% confidence intervals for simulated bladder data. \label{tab:simulation}}
\end{table}%\vskip18pt

<<simulation, echo=FALSE, fig.width=7, fig.height=5, fig.cap='Coverage probability of 95\\% confidence intervals for simulated bladder data. The coverage estimates based on the inversion method are colored blue.', fig.pos='!htb'>>=

cp <- c(0.89, 0.89, 0.89, 0.87, 0.89, 0.89, 0.90, 0.89, 0.88, 0.90, 0.92, 0.92,
        0.94, 0.93, 0.93, 0.92, 0.92, 0.94, 0.94, 0.93, 0.95, 0.95, 0.95, 0.94,
        0.95, 0.94, 0.94, 0.94, 0.94, 0.95, 0.95, 0.96, 0.95, 0.94, 0.94, 0.94,
        0.95, 0.95, 0.94, 0.94, 0.95, 0.95, 0.95, 0.94, 0.94, 0.95, 0.95, 0.95,
        0.94, 0.95)
m <- factor(rep(c(5, 10, 30, 50, 100), each = 10))
n <- factor(rep(c(5, 10, 30, 50, 100, 5, 10, 30, 50, 100), times = 5))
interval <- factor(rep(c("wald", "inversion"), each = 5, times = 5))
res <- data.frame(cp, m, n, interval)
xyplot(cp ~ m | n, groups = interval, type = "b", ylab = "Coverage", pch = 19,
#        auto.key = TRUE,
       panel = function(x, y, ...) {
         panel.grid()
         panel.abline(h = 0.95)
         panel.xyplot(x, y, ...)
       })

@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection[Using the lme4 package]{Using the \pkg{lme4} package}
\label{sec:lme4}
Implementation of the parametric bootstrap algorithm in Figure~\ref{fig:parboot} is relatively straight forward using the new \code{bootMer} function from the well-known \proglang{R} package \pkg{lme4} \citep{bates-lme4-2014} in conjunction with the \pkg{boot} package.

Since we will be using the \pkg{lme4} package, we need to refit the model using the \code{lmer} function:
<<>>=
library(lme4)  # assuming lme4 is already installed
fit.lme4 <- lmer(HD2 ~ volume + (0+1|subject) + (0+volume|subject),
                 data = bladder)
@
Theoretically, the parameter estimates from this model should be the same as those from \code{fit.nlme}; however, there are likely to be small numerical differences between the two.  For this reason, let us re-estimate $\widehat{x}_0$ using \code{fit.lme4}.  Since \code{invest} does not work on objects of class \code{lmer}, we have to do things manually:
<<>>=
fe <- unname(fixef(fit.lme4))  # fixed effects without dimnames attribute
(x0.est <- (500 - fe[1]) / fe[2])
@
Also, for convenience, we define the following function which estimates $\VAR\left[\mathcal{Y}|x\right] = \sigma_0^2 + x^2\sigma_1^2 + \sigma^2$ for a given value of $x$:
<<>>=
var.y <- function(object, x) {
  vc <- as.data.frame(lme4::VarCorr(object))$vcov
  vc[1] + vc[2]*x^2 + vc[3]
}
@
For example, to estimate $\sigma_0^2 = \widehat{\VAR}\left[\mathcal{Y}_0\right]$, we have \code{var.y(fit.lme4, x = x0.est)}, which gives \Sexpr{var.y(fit.lme4, x = x0.est)}, the same value used in the previous section.

Although we could easily compute all the bootstrap intervals previously discussed in one call to \code{bootMer} and \code{boot.ci}, we will discuss and compute each interval separately.

The following snippet of code generates $R = 9999$ bootstrap replicates of $\widehat{x}_0$, $Q_W$, and $Q_I$ according to the algorithm in Figure~\ref{fig:parboot}:
<<>>=
boot.fun <- function(.) {  # bootstrap function

  ## Point estimate
  var.y0.boot <- var.y(., x = x0.est)  # VAR[Y0]
  fe.boot <- unname(fixef(.))  # fixed effects
  if (all(getME(., "y") == bladder$HD2)) {
    y0.boot <- 500
  } else {
    y0.boot <- rnorm(1, 500, sqrt(var.y0.boot))
  }
  x0.boot <- (y0.boot - fe.boot[1])/fe.boot[2]

  ## Approximate variance
  covmat <- diag(3)
  covmat[1:2, 1:2] <- as.matrix(vcov(.))
  covmat[3, 3] <-  var.y0.boot
  params <- c("b0" = fe.boot[1], "b1" = fe.boot[2], "y0" = y0.boot)
  dm <- deltaMethod(params, g = "(y0 - b0)/b1",  vcov. = covmat)
  var.x0.boot <- dm$SE^2

  ## Approximate predictive pivot
  mu0.boot <-  as.numeric(crossprod(fe.boot, c(1, x0.est)))
  var.mu0.boot <- t(c(1, x0.est)) %*% as.matrix(vcov(.)) %*% c(1, x0.est)
  QI.boot <- (y0.boot - mu0.boot)/sqrt(var.y0.boot + var.mu0.boot)

  c(x0.boot, var.x0.boot, QI.boot)

}
pb <- bootMer(fit.lme4, boot.fun, nsim = 9999, seed = 105)  # run simulation
@
The \code{bootMer} function returns an object of class \code{boot} which can then be processed via the \pkg{boot} package to obtain the various bootstrap confidence intervals discussed earlier.  A basic summary of \code{pb} is given by
<<boot, fig.pos='!bth'>>=
library(boot)  # load boot package
summary(pb)
@
The estimated standard error and bias of $\widehat{x}_0$, based on \code{R = \Sexpr{summary(pb)$R[1]}} bootstrap replicates, are \Sexpr{summary(pb)$bootSE[1]} and \Sexpr{summary(pb)$bootBias[2]}, respectively.  The original estimate $\widehat{x}_0$ and the median of the bootstrap replicates are also given in the first row of the summary.  A graphical summary of the bootstrap simulation is given in Figure~\ref{fig:boot-plots}.  These graphs indicate that the sampling distributions of $\widehat{x}_0$, $Q_W$, and $Q_I$ are all approximately normal; hence, we would expect the bootstrap confidence intervals to be similar to the asymptotic methods based on the normal distribution discussed in Sections~\ref{sec:wald}-\ref{sec:inversion}.
<<boot-plots, echo=FALSE, fig.width=7, fig.height=4, out.width='\\linewidth', fig.cap='Graphical summary of bootstrap replicates. $R = 9,999$ bootstrap replicates of $\\widehat{x}_0$ (left), $Q_W$ (middle), and $Q_I$ (right).', fig.pos='!htb'>>=
## Draw histograms and normal Q-Q plots
x0.boot <- pb$t[, 1]
x0.stud <- (x0.boot - x0.est)/sqrt(pb$t[, 2])
QI.boot <- pb$t[, 3]
par(mfrow = c(2, 3), mar = c(4, 4, 0.1, 0.1), las = 1)
hist(x0.boot, br = 50, freq = FALSE, col = cols[1], border = "white",
     main = "", xlab = "")
abline(v = x0.est, lwd = 2)
legend("topleft", "Original", bty = "n")
hist(x0.stud, br = 50, freq = FALSE, col = cols[2], border = "white",
     main = "", xlab = "")
abline(v = 0, lwd = 2)
legend("topleft", "Wald", bty = "n")
hist(QI.boot, br = 50, freq = FALSE, col = cols[3], border = "white",
     main = "", xlab = "")
abline(v = 0, lwd = 2)
legend("topleft", "Inversion", bty = "n")
qqnorm(x0.boot, main = "", xlab = "Normal quantile", ylab = "Sample quantile",
       col = cols[1])
qqline(x0.boot)
qqnorm(x0.stud, main = "", xlab = "Normal quantile", ylab = "Sample quantile",
       col = cols[2])
qqline(x0.stud)
qqnorm(QI.boot, main = "", xlab = "Normal quantile", ylab = "Sample quantile",
       col = cols[3])
qqline(QI.boot)
@
To obtain the percentile and studentized $t$ intervals (see Sections~\ref{sec:percentile}-\ref{sec:student}), we can use the \pkg{boot} package function \code{boot.ci}:
<<>>=
boot.ci(pb, type = c("norm", "perc", "stud"))
@
For comparison, we also included the option to compute a bootstrap normal-approximation confidence interval.  This interval has the form
\[
  \left( \widehat{x}_0- \texttt{bootBias} \right) \pm z_{\alpha/2}\texttt{bootSE}
\]
where \code{bootBias} and \code{bootSE} can be found in the first row of \code{summary(pb)}.  In other words, it is just a Wald-type interval that uses a bias-corrected estimate of $x_0$, along with a bootstrap estimate of the standard error of $\widehat{x}_0$.  While this may be more accurate than the ordinary Wald-based interval \eqref{eqn:wald}, it may still not perform well in small sample sizes because of the strict normality assumption.  In this example, however, normality does not appear to be an issue.

The bootstrap adjusted inversion interval can be computed as easily as the ordinary inversion interval, except we need to supply \code{invest} with the estimated quantiles $\widehat{F}_{Q_I}\left(0.025\right)$ and $\widehat{F}_{Q_I}\left(0.975\right)$:
<<>>=
QI.boot <- pb$t[, 3]  # bootsrap replicates of Q_I
qvals <- quantile(QI.boot, c(0.025, 0.975))  # sample quantiles
invest(fit.nlme, y0 = 500, q1 = qvals[1], q2 = qvals[2])
@

All of the approximate 95\% confidence intervals we computed for the true volume of fluid are summarized in Table~\ref{tab:summary} below.

<<table setup, echo=FALSE>>=
wald <- invest(fit.nlme, y0 = 500, interval = "Wald")
inversion <- invest(fit.nlme, y0 = 500)
inversion.pb <- invest(fit.nlme, y0 = 500, q1 = qvals[1], q2 = qvals[2])
se.boot <- summary(pb)$bootSE[1]
boot.cis <- boot.ci(pb, type = c("perc", "stud"))
perc <- boot.cis$percent[4:5]
stud <- boot.cis$student[4:5]
lengths <- apply(rbind(c(wald$lower, wald$upper),
                       c(inversion$lower, inversion$upper), perc, stud,
                       c(inversion.pb$lower, inversion.pb$upper)), 1, diff)
@

\begin{table}[!htb]
\centering
\begin{tabular}{lcccc}
  \toprule
  Method                      &  Estimate & SE & 95\% Bounds & Length \\
  \midrule
  $CI_{wald}\left(x_0\right)$                 & \Sexpr{x0.est} & \Sexpr{wald$se} & (\Sexpr{wald$lower}, \Sexpr{wald$upper}) & \Sexpr{lengths[1]} \\
  $CI_{inv}\left(x_0\right)$            & \Sexpr{x0.est} &  ---            & (\Sexpr{inversion$lower}, \Sexpr{inversion$upper}) & \Sexpr{lengths[2]} \\
  $CI_{percentile}\left(x_0\right)^\star$     & \Sexpr{x0.est} & \Sexpr{se.boot} & (\Sexpr{perc[1]}, \Sexpr{perc[2]}) & \Sexpr{lengths[3]} \\
  $CI_{wald}\left(x_0\right)^\star$           & \Sexpr{x0.est} & \Sexpr{se.boot} & (\Sexpr{stud[1]}, \Sexpr{stud[2]}) & \Sexpr{lengths[4]} \\
  $CI_{inv}\left(x_0\right)^\star$      & \Sexpr{x0.est} & \Sexpr{se.boot} & (\Sexpr{inversion.pb$lower}, \Sexpr{inversion.pb$upper}) & \Sexpr{lengths[5]} \\
%   Bayesian (uniform prior)    & 8.445     & 1.825 & (4.629, 11.921) & 7.292 \\
  \bottomrule
\end{tabular}
\caption{Summary of results for the bladder volume example.  A $\star$ symbol indicates a parametric bootstrap-based confidence interval. \label{tab:summary}}
\end{table}

Suppose instead that we observed a new value \code{HD = 60} and we wish to estimate the true volume of liquid in the patients bladder using the original data.  The point estimate is easily obtained using the quadratic formula: $\widehat{x}_0 = \Sexpr{11.10487}$.  Recall, from Figure~\ref{fig:spaghetti} that each patient has a slightly nonlinear trajectory; thus, there is no reason to expect the sampling distribution of $\widehat{x}_0$ to be normal, or even symmetric in this case.  To see that this is indeed the case, we applied the parametric bootstrap.  The results are summarized in Figure~\ref{fig:boot-plots-2}.  Clearly, the Wald-based confidence interval \eqref{eqn:wald} will not be accurate in this case.  However, the approximate predictive pivot used in the inversion interval \eqref{eqn:inversion} appears reasonably normal.  Thus, our recommendation is that, if the number of subjects $m$ is reasonably large (say $m \ge 25$) and the bootstrap replicates are approximately normal (see Figure~\ref{fig:boot-plots}), then the Wald-based and inversion methods are useful.  Otherwise, it is probably best to stick with the parametric bootstrap confidence intervals or, if prior information is available, adopt a fully Bayesian approach.
<<boot-plots-2, echo=FALSE, fig.width=7, fig.height=4, out.width='\\linewidth', fig.cap='Graphical summary of bootstrap replicates. $R = 9,999$ bootstrap replicates of $\\widehat{x}_0$ (left), $Q_W$ (middle), and $Q_I$ (right).', fig.pos='!htb'>>=
## Load data
# load("/home/w108bmg/Desktop/Dropbox/pb-original.RData")

## Draw histograms and normal Q-Q plots
x0.boot2 <- pb$t[, 1]
x0.stud2 <- (x0.boot2 - 11.10487)/sqrt(pb$t[, 2])
QI.boot2 <- pb$t[, 3]
par(mfrow = c(2, 3), mar = c(4, 4, 0.1, 0.1), las = 1)
hist(x0.boot2, br = 50, freq = FALSE, col = cols[1], border = "white",
     main = "", xlab = "")
abline(v = 11.10487, lwd = 2)
legend("topleft", "Original", bty = "n")
hist(x0.stud2, br = 50, freq = FALSE, col = cols[2], border = "white",
     main = "", xlab = "")
abline(v = 0, lwd = 2)
legend("topleft", "Wald", bty = "n")
hist(QI.boot2, br = 50, freq = FALSE, col = cols[3], border = "white",
     main = "", xlab = "")
abline(v = 0, lwd = 2)
legend("topleft", "Inversion", bty = "n")
qqnorm(x0.boot2, main = "", xlab = "Normal quantile", ylab = "Sample quantile",
       col = cols[1])
qqline(x0.boot2)
qqnorm(x0.stud2, main = "", xlab = "Normal quantile", ylab = "Sample quantile",
       col = cols[2])
qqline(x0.stud2)
qqnorm(QI.boot2, main = "", xlab = "Normal quantile", ylab = "Sample quantile",
       col = cols[3])
qqline(QI.boot2)
@

%' %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%' %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%' \subsection{Comparison with MCMC Results}
%'
%' <<bayesian-model, echo=FALSE, cache=TRUE, warning=FALSE, message=FALSE, results='hide'>>=
%'
%' ## Load libraries
%' library(rjags)
%'
%' ## Path to JAGS model file
%' path <- "/home/w108bmg/Desktop/Dropbox/Projects/greenwell_linear_2014 (JSS)"
%' name <- "bladder-transformed.txt"
%' model.file <- paste(path, name, sep = "/")
%'
%' ## Data
%' data.list <- with(bladder, list(y = HD2, x = volume, subject = subject, y0 = 500,
%'                                 n = length(HD2), m = length(unique(subject))))
%'
%' ## Initial values for chain
%' fit <- lmer(HD2 ~ I(volume-mean(volume)) + (0+1|subject) + (0+volume|subject),
%'             data = bladder)
%' fe <- fixef(fit); vc <- as.data.frame(VarCorr(fit))$sdcor
%' inits.list <- list(mu.a = fe[1], mu.b = fe[2], sigma.a = vc[1], sigma.b = vc[2],
%'                    sigma.y = vc[3],
%'                    x0 = (500 - fe[1])/fe[2] + mean(volume),
%'                    .RNG.name = "base::Mersenne-Twister", .RNG.seed = 1)
%'
%' ## JAGS model
%' sim <- jags.model(model.file, data = data.list, inits = inits.list,
%'                   n.adapt = 10000)
%' update(sim, n.iter = 10000)  # burn-in
%' x0.coda <- coda.samples(sim, "x0", n.iter = 99990, thin = 10)
%' x0.post <- as.numeric(as.matrix(x0.coda))
%' dens.x0 <- density(x0.post)
%' x0.mode <- dens.x0$x[which.max(dens.x0$y)]
%'
%' @
%'
%' %Rather than simulating the coverage probability for the parametric bootstrap intervals (which would take a really long time),
%'
%' Just for fun, let us compare the parametric bootstrap results from the transformed data against those from a simple Bayesian analysis using modern MCMC techniques.  In particular, we are interested in comparing the posterior distribution of $x_0$ from a Bayesian model against the bootstrap distribution of $\widehat{x}_0$ obtained in Section~\ref{sec:lme4}.  Our Bayesian analysis was compiled using \proglang{JAGS} \citep{plummer-jags-2003} via the \proglang{R} package \pkg{rjags} \citep{plummer-rjags-2014}.  Our JAGS model follows closely with the approach outlined in \citet{hamada-bayesian-2003}.
%'
%' Let $\pi(\cdot)$ denote a probability density function.  Following \citet{hoadley-bayesian-1970}, we assume that the calibration experiment contains no information about $x_0$ and that the priors for $x_0$ and the calibration experiment are independent; that is,
%' \begin{equation*}
%'   \pi(x_0, \bm{\beta}, \bm{b}, \sigma, \bm{\theta}) = \pi(x_0)\pi(\bm{\beta}, \bm{b}, \sigma, \bm{\theta}).
%' \end{equation*}
%' The (unnormalized) posterior density of $(x_0, \bm{\beta}, \bm{b}, \sigma, \bm{\theta})$ is given by
%' \begin{align*}
%'   \pi(x_0, \bm{\beta}, \bm{b}, \sigma, \bm{\theta} | \mathbf{data}) &= \pi(x_0, \bm{\beta}, \bm{b}, \sigma, \bm{\theta} | \bm{y}, y_0) \\
%'   &\propto \pi(\bm{y}, y_0 | x_0, \bm{\beta}, \bm{b}, \sigma, \bm{\theta})\pi(x_0, \bm{\beta}, \bm{b}, \sigma, \bm{\theta}) \\
%'   &\propto \pi(y_0 | x_0, \bm{\beta}, \sigma, \bm{\theta})\pi(\bm{y} | \bm{\beta}, \bm{b}, \sigma, \bm{\theta})\pi(\bm{\beta}, \bm{b}, \sigma, \bm{\theta})\pi(x_0) \\
%'   &\propto \pi(y_0 | x_0, \bm{\beta}, \sigma, \bm{\theta})\pi(\bm{y} | \bm{\beta}, \bm{b}, \sigma, \bm{\theta})\pi(\bm{\beta})\pi(\bm{b}|\bm{\theta})\pi(\sigma)\pi(\bm{\theta})\pi(x_0),
%' \end{align*}
%' where $\bm{y}$ and $y_0$ represent the observed data from the first and second stages of the calibration experiment, respectively.
%'
%' The LMM \eqref{eqn:lmm-stacked} already has $\bm{b} \sim \mathcal{N}\left(0, \bm{D}\left(\bm{\theta}\right)\right)$.  A fully Bayesian approach, however, requires a prior distribution on the parameters $(\bm{\beta}, \sigma^2, \bm{\theta}, x_0)$; here, $\bm{\theta} = \left(\theta_0^2, \theta_1^2\right)$.  Rather than assigning a prior to the variance components, we will instead assign priors to the standard deviations, which in turn induces priors on the variance components.  Following standard convention (and since we do not have any available prior information), we assume, a priori, that the fixed-effects are independent and assign vague, independent priors to $\left(\bm{\beta}, \sigma, \theta_0, \theta_1\right)$.  We used the proper but diffuse priors
%' \begin{align*}
%'   \beta_i &\sim \mathcal{N}\left(0, 10000\right), \quad i = 0, 1, \\
%'   \sigma &\sim \mathcal{U}\left(0, 100\right), \\
%'   \theta_j &\sim \mathcal{U}\left(0, 100\right), \quad j = 0, 1.
%' \end{align*}
%' In addition, since the unknown volume must be positive, we assigned $x_0$ a truncated normal prior that is restricted to the positive real numbers.  We obtained 9,999 draws after discarding the first 1,000 for burn-in.  Some thinning was required to reduce autocorrelation in the posterior samples for the variance components and $x_0$.  A small summary of the marginal posterior of $x_0$ is given below.
%' <<echo=FALSE>>=
%'
%' ## Posterior summary
%' c(Mean = mean(x0.post), SD = sd(x0.post), Median = median(x0.post),
%'   Mode = x0.mode, quantile(x0.post, c(0.025, 0.975)))
%'
%' @
%'
%' A kernel density estimate of the posterior draws is shown in the left panel of Figure~\ref{fig:boot-post} along with a kernel density estimate of the bootstrap replicates of $\widehat{x}_0$ obtained in the previous section.  The two distributions are comparable, but there is a slight discrepancy in the tails (the posterior is slightly more positively skewed).  A 95\% Bayesian credible interval for $x_0$ is $(\Sexpr{as.numeric(quantile(x0.post, 0.025))}, \Sexpr{as.numeric(quantile(x0.post, 0.975))})$, which is slightly shorter than the asymptotic and bootstrap confidence intervals given in Table~\ref{tab:summary}.
%'
%' <<boot-post, echo=FALSE, fig.width=7, fig.height=4, out.width='\\linewidth', fig.cap='Bayesian and parametric bootstrap results. \\textit{Left}: Kernel density estimate of  9,999 bootstrap replicates (black line) and a kernel density estimate of 9,999 samples from the marginal posterior (purple line). The vertical line indicates the position of the original estimate $\\widehat{x}_0$. \\textit{Right}: Quantile-quantile plots of the bootstrap and posterior samples.', fig.pos='!hbt'>>=
%'
%' ## Draw densities and Q-Q plot
%' par(mfrow = c(1, 2))
%' x0.boot.dens <- density(x0.boot)
%' x0.post.dens <- density(x0.post)
%' plot(x0.post.dens, col = cols[4], lwd = 2, xlab = "", main = "",
%'      xlim = extendrange(range(x0.boot), f = 0.1),
%'      ylim = c(0, max(c(x0.post.dens$y, x0.boot.dens$y))))
%' lines(x0.boot.dens, lwd = 2)
%' abline(v = x0.est, lwd = 2)
%' qqplot(x0.boot, x0.post, xlab = "Bootstrap", ylab = "Posterior")
%' abline(0, 1, lty = 2)
%'
%' @

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

We have discussed a number of confidence interval procedures for statistical calibration in linear models with random coefficients with a single level of grouping.  We have described two \proglang{R} packages for implementing these procedures: \pkg{investr} and \pkg{lme4}.  The \pkg{investr} package can be used for obtaining the asymptotic confidence intervals (i.e., the Wald-based and inversion confidence intervals).  We also showed how the \pkg{lme4} package can be used to obtain calibration intervals based on a parametric bootstrap using the recently added \code{bootMer} function.  Future work will likely extend the methods discussed in this paper to more complicated cases such as nonlinear mixed-effects models and multi-level hierarchical models (i.e., more than one grouping variable).


\bibliography{greenwell-linear-2014}

\end{document}
